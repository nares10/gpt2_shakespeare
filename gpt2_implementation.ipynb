{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5WvrP6tQAqtyga8QCVcrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nares10/gpt2_shakespeare/blob/main/gpt2_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nq0oUDbf4zyG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tiny Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "import requests\n",
        "text = requests.get(url).text\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0oEaKQUR5BFj",
        "outputId": "08b085c3-98c6-4ed4-a253-51597549d935"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "print(\"vocab size: \",vocab_size)\n",
        "print(\"unique character: \", chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3-Jm1Dx5LB3",
        "outputId": "901dd85b-3907-4bcf-a4d1-7cee8d01e2f1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size:  65\n",
            "unique character:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(s):\n",
        "    return [char_to_idx[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([idx_to_char[i] for i in l])"
      ],
      "metadata": {
        "id": "sZqxaMuL5d_k"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "block_size = 128  # context length\n",
        "batch_size = 32\n",
        "embed_dim = 256\n",
        "n_heads = 8\n",
        "n_layers = 12\n",
        "hidden_dim = 1024\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 5\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "FR51KGZL6Gvi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Dataset\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, text, block_size):\n",
        "        data = torch.tensor(encode(text), dtype=torch.long)\n",
        "        self.inputs = [data[i:i + block_size] for i in range(len(data) - block_size)]\n",
        "        self.targets = [data[i + 1:i + block_size + 1] for i in range(len(data) - block_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "PUCyFA8Z6N-h"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ShakespeareDataset(text, block_size)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "fCd2WuH36Rz_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        q = self.q_linear(x)  # (batch, seq_len, embed_dim)\n",
        "        k = self.k_linear(x)\n",
        "        v = self.v_linear(x)\n",
        "\n",
        "        # Reshape for multi-head attention: (batch, n_heads, seq_len, head_dim)\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            # mask shape: (seq_len, seq_len); expand to (batch, n_heads, seq_len, seq_len)\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v)  # (batch, n_heads, seq_len, head_dim)\n",
        "\n",
        "        # Concatenate attention heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        output = self.out_linear(attn_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "v2bZeEA09Dmq"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, n_heads)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = self.ln1(x + self.dropout(attn_out))\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_out))\n",
        "        return x"
      ],
      "metadata": {
        "id": "W2s38qns9Sx0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, hidden_dim, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, block_size, embed_dim))\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderBlock(embed_dim, n_heads, hidden_dim, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        # Embed tokens and add positional embeddings\n",
        "        #x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :min(seq_len, self.block_size), :]\n",
        "\n",
        "        # Create causal mask: (seq_len, seq_len) with ones in the lower triangle\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).bool()\n",
        "        # Pass through all transformer decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ybLuADQd9gn_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2(vocab_size, embed_dim, n_heads, n_layers, hidden_dim, block_size).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "m7yVI3tp9hXn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        # reshape logits and targets for computing loss\n",
        "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "YLzLyppV9lnr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, max_length=200):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_seq[:, -model.block_size:])\n",
        "\n",
        "            #logits = model(input_seq)\n",
        "            # Select the token with the highest probability\n",
        "            next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "    return decode(input_seq.squeeze().tolist())\n",
        "\n",
        "print(generate_text(model, \"JULIET:\", 200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvHG3x-W9rFV",
        "outputId": "f1f82e1b-e510-4da3-e159-fa5bf18ce5b3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "And then the shall be so much since as the sea,\n",
            "As the shadow of the streets and the streets of his head,\n",
            "And then the sea is so fair and shall be so.\n",
            "\n",
            "KING RICHARD III:\n",
            "Then shall we shall be so far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model parameters\n",
        "torch.save(model.state_dict(), \"gpt2_shakespeare.pth\")"
      ],
      "metadata": {
        "id": "VZk2HGmQ-Zqy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Later, to load the model parameters back:\n",
        "model = GPT2(vocab_size, embed_dim, n_heads, n_layers, hidden_dim, block_size).to(device)\n",
        "model.load_state_dict(torch.load(\"gpt2_shakespeare.pth\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "a6MNydsK-hI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c37bd5-6c53-4af0-815c-65b10a67efe6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-a9c2612d8571>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"gpt2_shakespeare.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (embedding): Embedding(65, 256)\n",
              "  (layers): ModuleList(\n",
              "    (0-11): 12 x TransformerDecoderBlock(\n",
              "      (self_attn): MultiHeadSelfAttention(\n",
              "        (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (out_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "      )\n",
              "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=256, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9v0o6ZYEGhm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}